{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5404d5bd",
   "metadata": {},
   "source": [
    "# TP Final — Phase 1 : Ingestion Bronze (PostgreSQL → MinIO)\n",
    "\n",
    "**But** : ingerer des donnees brutes depuis PostgreSQL (Northwind) vers la zone **Bronze** (MinIO/S3) selon l’architecture Medallion.\n",
    "\n",
    "## Attendus couverts\n",
    "- Lecture generique d’une table PostgreSQL\n",
    "- Ecriture Parquet dans `s3a://bronze/<table_name>/`\n",
    "- Partitionnement par **date d’ingestion** au format `YYYY-MM-DD`\n",
    "- Ajout des metadonnees techniques obligatoires :\n",
    "  - `_ingestion_timestamp`\n",
    "  - `_source_system` = `postgresql`\n",
    "  - `_table_name`\n",
    "\n",
    "> Remarque : on ajoute aussi `_ingestion_date` pour porter la partition (colonne technique)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088c7fd",
   "metadata": {},
   "source": [
    "## 1) Contexte d’execution\n",
    "Ce notebook est fait pour etre execute **dans le JupyterLab de l’environnement Docker** (service `jupyter-spark`).\n",
    "\n",
    "- PostgreSQL dans Docker : `jdbc:postgresql://postgres:5432/app`\n",
    "- MinIO dans Docker : `http://minio:9000`\n",
    "- Buckets attendus : `bronze`, `silver`, `gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d58b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK\n",
      "S3A endpoint = http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# SparkSession avec config MinIO + drivers (au besoin)\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"TP Final - Phase 1 Bronze\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Pour permettre overwrite uniquement de la partition du jour\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "print(\"Spark OK\")\n",
    "print(\"S3A endpoint =\", spark.sparkContext._jsc.hadoopConfiguration().get(\"fs.s3a.endpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc3233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion date = 2026-01-16\n"
     ]
    }
   ],
   "source": [
    "# Configuration JDBC PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/app\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "# Date d’ingestion (partition) au format YYYY-MM-DD\n",
    "ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(\"Ingestion date =\", ingestion_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4330fb",
   "metadata": {},
   "source": [
    "## 2) Fonction d’ingestion generique (PostgreSQL → Bronze)\n",
    "Cette fonction :\n",
    "- lit la table via JDBC\n",
    "- ajoute les colonnes techniques obligatoires\n",
    "- ecrit en Parquet dans `s3a://bronze/<table_name>/<YYYY-MM-DD>/` (partitionnement par dossier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b4cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_table_to_bronze(table_name: str, *, ingestion_date: str, base_path: str = \"s3a://bronze\") -> dict:\n",
    "    \"\"\"\n",
    "    Ingerer une table PostgreSQL vers la zone Bronze sur MinIO.\n",
    "\n",
    "    Ecrit en Parquet et partitionne par date d’ingestion (YYYY-MM-DD) via la structure de dossiers.\n",
    "    Ajoute les metadonnees techniques :\n",
    "      - _ingestion_timestamp\n",
    "      - _source_system = 'postgresql'\n",
    "      - _table_name\n",
    "      - _ingestion_date\n",
    "\n",
    "    Retourne un petit dictionnaire de stats.\n",
    "    \"\"\"\n",
    "    ingestion_ts_col = F.current_timestamp()\n",
    "\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n",
    "    row_count = df.count()\n",
    "\n",
    "    df_out = (df\n",
    "        .withColumn(\"_ingestion_timestamp\", ingestion_ts_col)\n",
    "        .withColumn(\"_source_system\", F.lit(\"postgresql\"))\n",
    "        .withColumn(\"_table_name\", F.lit(table_name))\n",
    "        .withColumn(\"_ingestion_date\", F.lit(ingestion_date))\n",
    "    )\n",
    "\n",
    "    target_path = f\"{base_path}/{table_name}/{ingestion_date}\"\n",
    "\n",
    "    df_out.write.mode(\"overwrite\").parquet(target_path)\n",
    "\n",
    "    return {\n",
    "        \"table\": table_name,\n",
    "        \"rows\": row_count,\n",
    "        \"path\": target_path,\n",
    "        \"partition\": ingestion_date,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fcc38",
   "metadata": {},
   "source": [
    "## 3) Execution Phase 1 : tables obligatoires (+ bonus)\n",
    "Tables obligatoires : `customers`, `orders`, `order_details`, `products`\n",
    "\n",
    "Tables bonus (si dispo) : `employees`, `suppliers`, `categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3892084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1 - INGESTION BRONZE ===\n",
      "[OK] customers -> 91 lignes -> s3a://bronze/customers/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] orders -> 830 lignes -> s3a://bronze/orders/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] order_details -> 2155 lignes -> s3a://bronze/order_details/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] products -> 77 lignes -> s3a://bronze/products/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] employees -> 9 lignes -> s3a://bronze/employees/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] suppliers -> 29 lignes -> s3a://bronze/suppliers/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] categories -> 8 lignes -> s3a://bronze/categories/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "\n",
      "Resume OK :\n",
      "  - customers: 91 lignes\n",
      "  - orders: 830 lignes\n",
      "  - order_details: 2155 lignes\n",
      "  - products: 77 lignes\n",
      "  - employees: 9 lignes\n",
      "  - suppliers: 29 lignes\n",
      "  - categories: 8 lignes\n"
     ]
    }
   ],
   "source": [
    "required_tables = [\"customers\", \"orders\", \"order_details\", \"products\"]\n",
    "bonus_tables = [\"employees\", \"suppliers\", \"categories\"]\n",
    "tables_to_ingest = required_tables + bonus_tables\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "print(\"=== PHASE 1 - INGESTION BRONZE ===\")\n",
    "for table in tables_to_ingest:\n",
    "    try:\n",
    "        stats = ingest_table_to_bronze(table, ingestion_date=ingestion_date)\n",
    "        results.append(stats)\n",
    "        print(f\"[OK] {stats['table']} -> {stats['rows']} lignes -> {stats['path']} (_ingestion_date={stats['partition']})\")\n",
    "    except Exception as e:\n",
    "        errors.append({\"table\": table, \"error\": str(e)})\n",
    "        print(f\"[ERREUR] {table} : {e}\")\n",
    "\n",
    "print(\"\\nResume OK :\")\n",
    "for r in results:\n",
    "    print(f\"  - {r['table']}: {r['rows']} lignes\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nResume erreurs :\")\n",
    "    for err in errors:\n",
    "        print(f\"  - {err['table']}: {err['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329dd24",
   "metadata": {},
   "source": [
    "## 4) Verifications rapides\n",
    "On relit une table depuis Bronze et on verifie la presence des colonnes techniques + la partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f623ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes manquantes : set()\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "|_ingestion_timestamp      |_source_system|_table_name|_ingestion_date|\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "+---------------+-----+\n",
      "|_ingestion_date|count|\n",
      "+---------------+-----+\n",
      "|2026-01-16     |91   |\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple : verifier customers (lecture de la partition du jour)\n",
    "df_bronze_customers = spark.read.parquet(f\"s3a://bronze/customers/{ingestion_date}\")\n",
    "\n",
    "expected_cols = {\"_ingestion_timestamp\", \"_source_system\", \"_table_name\", \"_ingestion_date\"}\n",
    "missing = expected_cols - set(df_bronze_customers.columns)\n",
    "print(\"Colonnes manquantes :\", missing)\n",
    "\n",
    "df_bronze_customers.select(\n",
    "    \"_ingestion_timestamp\",\n",
    "    \"_source_system\",\n",
    "    \"_table_name\",\n",
    "    \"_ingestion_date\",\n",
    ").show(5, truncate=False)\n",
    "\n",
    "df_bronze_customers.groupBy(\"_ingestion_date\").count().orderBy(\"_ingestion_date\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
