{"cells": [{"cell_type": "markdown", "id": "5404d5bd", "metadata": {}, "source": ["# TP Final — Phase 1 : Ingestion Bronze (PostgreSQL → MinIO)\n", "\n", "**But** : ingerer des donnees brutes depuis PostgreSQL (Northwind) vers la zone **Bronze** (MinIO/S3) selon l’architecture Medallion.\n", "\n", "## Attendus couverts\n", "- Lecture generique d’une table PostgreSQL\n", "- Ecriture Parquet dans `s3a://bronze/<table_name>/`\n", "- Partitionnement par **date d’ingestion** au format `YYYY-MM-DD`\n", "- Ajout des metadonnees techniques obligatoires :\n", "  - `_ingestion_timestamp`\n", "  - `_source_system` = `postgresql`\n", "  - `_table_name`\n", "\n", "> Remarque : on ajoute aussi `_ingestion_date` pour porter la partition (colonne technique)."]}, {"cell_type": "markdown", "id": "9088c7fd", "metadata": {}, "source": ["## 1) Contexte d’execution\n", "Ce notebook est fait pour etre execute **dans le JupyterLab de l’environnement Docker** (service `jupyter-spark`).\n", "\n", "- PostgreSQL dans Docker : `jdbc:postgresql://postgres:5432/app`\n", "- MinIO dans Docker : `http://minio:9000`\n", "- Buckets attendus : `bronze`, `silver`, `gold`"]}, {"cell_type": "code", "execution_count": 8, "id": "6d58b170", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Spark OK\n", "S3A endpoint = http://minio:9000\n"]}], "source": ["from pyspark.sql import SparkSession\n", "from pyspark.sql import functions as F\n", "from datetime import datetime\n", "\n", "# SparkSession avec config MinIO + drivers (au besoin)\n", "spark = (SparkSession.builder\n", "    .appName(\"TP Final - Phase 1 Bronze\")\n", "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n", "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n", "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n", "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n", "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n", "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n", "    .getOrCreate())\n", "\n", "# Pour permettre overwrite uniquement de la partition du jour\n", "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n", "\n", "print(\"Spark OK\")\n", "print(\"S3A endpoint =", spark.sparkContext._jsc.hadoopConfiguration().get(\"fs.s3a.endpoint\"))"]}, {"cell_type": "code", "execution_count": 9, "id": "8cc3233e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Ingestion date = 2026-01-16\n"]}], "source": ["# Configuration JDBC PostgreSQL\n", "jdbc_url = \"jdbc:postgresql://postgres:5432/app\"\n", "jdbc_properties = {\n", "    \"user\": \"postgres\",\n", "    \"password\": \"postgres\",\n", "    \"driver\": \"org.postgresql.Driver\",\n", "}\n", "\n", "# Date d’ingestion (partition) au format YYYY-MM-DD\n", "ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n", "print(\"Ingestion date =", ingestion_date)\n"]}, {"cell_type": "markdown", "id": "ed4330fb", "metadata": {}, "source": ["## 2) Fonction d’ingestion generique (PostgreSQL → Bronze)\n", "Cette fonction :\n", "- lit la table via JDBC\n", "- ajoute les colonnes techniques obligatoires\n", "- ecrit en Parquet dans `s3a://bronze/<table_name>/<YYYY-MM-DD>/` (partitionnement par dossier)\n"]}, {"cell_type": "code", "execution_count": 10, "id": "e1b4cc1e", "metadata": {}, "outputs": [], "source": ["def ingest_table_to_bronze(table_name: str, *, ingestion_date: str, base_path: str = \"s3a://bronze\") -> dict:\n", "    \"\"\"\n", "    Ingerer une table PostgreSQL vers la zone Bronze sur MinIO.\n", "\n", "    Ecrit en Parquet et partitionne par date d’ingestion (YYYY-MM-DD) via la structure de dossiers.\n", "    Ajoute les metadonnees techniques :\n", "      - _ingestion_timestamp\n", "      - _source_system = 'postgresql'\n", "      - _table_name\n", "      - _ingestion_date\n", "\n", "    Retourne un petit dictionnaire de stats.\n", "    \"\"\"\n", "    ingestion_ts_col = F.current_timestamp()\n", "\n", "    df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n", "    row_count = df.count()\n", "\n", "    df_out = (df\n", "        .withColumn(\"_ingestion_timestamp\", ingestion_ts_col)\n", "        .withColumn(\"_source_system\", F.lit(\"postgresql\"))\n", "        .withColumn(\"_table_name\", F.lit(table_name))\n", "        .withColumn(\"_ingestion_date\", F.lit(ingestion_date))\n", "    )\n", "\n", "    target_path = f\"{base_path}/{table_name}/{ingestion_date}\"\n", "\n", "    df_out.write.mode(\"overwrite\").parquet(target_path)\n", "\n", "    return {\n", "        \"table\": table_name,\n", "        \"rows\": row_count,\n", "        \"path\": target_path,\n", "        \"partition\": ingestion_date,\n", "    }\n"]}, {"cell_type": "markdown", "id": "580fcc38", "metadata": {}, "source": ["## 3) Execution Phase 1 : tables obligatoires (+ bonus)\n", "Tables obligatoires : `customers`, `orders`, `order_details`, `products`\n", "\n", "Tables bonus (si dispo) : `employees`, `suppliers`, `categories`"]}, {"cell_type": "code", "execution_count": 11, "id": "a3892084", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["=== PHASE 1 - INGESTION BRONZE ===\n", "[OK] customers -> 91 lignes -> s3a://bronze/customers/2026-01-16 (_ingestion_date=2026-01-16)\n", "[OK] orders -> 830 lignes -> s3a://bronze/orders/2026-01-16 (_ingestion_date=2026-01-16)\n", "[OK] order_details -> 2155 lignes -> s3a://bronze/order_details/2026-01-16 (_ingestion_date=2026-01-16)\n", "[OK] products -> 77 lignes -> s3a://bronze/products/2026-01-16 (_ingestion_date=2026-01-16)\n", "[OK] employees -> 9 lignes -> s3a://bronze/employees/2026-01-16 (_ingestion_date=2026-01-16)\n", "[OK] suppliers -> 29 lignes -> s3a://bronze/suppliers/2026-01-16 (_ingestion_date=2026-01-16)\n", "[OK] categories -> 8 lignes -> s3a://bronze/categories/2026-01-16 (_ingestion_date=2026-01-16)\n", "\n", "Resume OK :\n", "  - customers: 91 lignes\n", "  - orders: 830 lignes\n", "  - order_details: 2155 lignes\n", "  - products: 77 lignes\n", "  - employees: 9 lignes\n", "  - suppliers: 29 lignes\n", "  - categories: 8 lignes\n"]}], "source": ["required_tables = [\"customers\", \"orders\", \"order_details\", \"products\"]\n", "bonus_tables = [\"employees\", \"suppliers\", \"categories\"]\n", "tables_to_ingest = required_tables + bonus_tables\n", "\n", "results = []\n", "errors = []\n", "\n", "print(\"=== PHASE 1 - INGESTION BRONZE ===\")\n", "for table in tables_to_ingest:\n", "    try:\n", "        stats = ingest_table_to_bronze(table, ingestion_date=ingestion_date)\n", "        results.append(stats)\n", "        print(f\"[OK] {stats['table']} -> {stats['rows']} lignes -> {stats['path']} (_ingestion_date={stats['partition']})\")\n", "    except Exception as e:\n", "        errors.append({\"table\": table, \"error\": str(e)})\n", "        print(f\"[ERREUR] {table} : {e}\")\n", "\n", "print(\"\nResume OK :\")\n", "for r in results:\n", "    print(f\"  - {r['table']}: {r['rows']} lignes\")\n", "\n", "if errors:\n", "    print(\"\nResume erreurs :\")\n", "    for err in errors:\n", "        print(f\"  - {err['table']}: {err['error']}\")"]}, {"cell_type": "markdown", "id": "7329dd24", "metadata": {}, "source": ["## 4) Verifications rapides\n", "On relit une table depuis Bronze et on verifie la presence des colonnes techniques + la partition."]}, {"cell_type": "code", "execution_count": 12, "id": "97f623ed", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Colonnes manquantes : set()\n", "+--------------------------+--------------+-----------+---------------+\n", "|_ingestion_timestamp      |_source_system|_table_name|_ingestion_date|\n", "+--------------------------+--------------+-----------+---------------+\n", "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n", "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n", "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n", "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n", "|2026-01-16 09:42:31.851842|postgresql    |customers  |2026-01-16     |\n", "+--------------------------+--------------+-----------+---------------+\n", "only showing top 5 rows\n", "+---------------+-----+\n", "|_ingestion_date|count|\n", "+---------------+-----+\n", "|2026-01-16     |91   |\n", "+---------------+-----+\n", "\n"]}], "source": ["# Exemple : verifier customers (lecture de la partition du jour)\n", "df_bronze_customers = spark.read.parquet(f\"s3a://bronze/customers/{ingestion_date}\")\n", "\n", "expected_cols = {\"_ingestion_timestamp\", \"_source_system\", \"_table_name\", \"_ingestion_date\"}\n", "missing = expected_cols - set(df_bronze_customers.columns)\n", "print(\"Colonnes manquantes :\", missing)\n", "\n", "df_bronze_customers.select(\n", "    \"_ingestion_timestamp\",\n", "    \"_source_system\",\n", "    \"_table_name\",\n", "    \"_ingestion_date\",\n", ").show(5, truncate=False)\n", "\n", "df_bronze_customers.groupBy(\"_ingestion_date\").count().orderBy(\"_ingestion_date\").show(truncate=False)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Phase 2 — Transformations Silver (Bronze → Silver)\n", "\n", "Les cellules suivantes s'exécutent après la Phase 1 dans le même kernel. Elles lisent les partitions Bronze écrites ci‑dessus, appliquent nettoyage / typage / enrichissements et écrivent la sortie dans s3a://silver/<table>/<ingestion_date>/."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fonctions utilitaires pour Phase 2\n", "from pyspark.sql import types as T\n", "from pyspark.sql import functions as F\n", "\n", "def read_bronze(table_name: str, ingestion_date: str):\n", "    path = f\"s3a://bronze/{table_name}/{ingestion_date}\"\n", "    return spark.read.parquet(path)\n", "\n", "def write_silver(df, table_name: str, ingestion_date: str, base_path: str = \"s3a://silver\"):\n", "    df_out = (df\n", "        .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n", "        .withColumn(\"_source_system\", F.lit(\"bronze\"))\n", "        .withColumn(\"_table_name\", F.lit(table_name))\n", "        .withColumn(\"_ingestion_date\", F.lit(ingestion_date))\n", "    )\n", "    target_path = f\"{base_path}/{table_name}/{ingestion_date}\"\n", "    row_count = df_out.count()\n", "    df_out.write.mode(\"overwrite\").parquet(target_path)\n", "    return {\"table\": table_name, \"rows\": row_count, \"path\": target_path, \"partition\": ingestion_date}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Normalisation automatique des noms de colonnes (prise en charge de variantes)\n", "def normalize_cols(df):\n", "    # map possible variants to canonical snake_case names\n", "    col_map = {}\n", "    cols = set(df.columns)\n", "    # customers common columns\n", "    if 'contactname' in cols and 'contact_name' not in cols:\n", "        col_map['contactname'] = 'contact_name'\n", "    if 'companyname' in cols and 'company_name' not in cols:\n", "        col_map['companyname'] = 'company_name'\n", "    if 'contacttitle' in cols and 'contact_title' not in cols:\n", "        col_map['contacttitle'] = 'contact_title'\n", "    if 'postalcode' in cols and 'postal_code' not in cols:\n", "        col_map['postalcode'] = 'postal_code'\n", "    # product columns\n", "    if 'productname' in cols and 'product_name' not in cols:\n", "        col_map['productname'] = 'product_name'\n", "    if 'unitprice' in cols and 'unit_price' not in cols:\n", "        col_map['unitprice'] = 'unit_price'\n", "    if col_map:\n", "        for src, dst in col_map.items():\n", "            df = df.withColumnRenamed(src, dst)\n", "    return df\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Transformations Silver par table (utilisent normalize_cols pour être robustes)\n", "def customers_silver(ingestion_date: str):\n", "    df = read_bronze('customers', ingestion_date)\n", "    df = normalize_cols(df)\n", "    df2 = (df\n", "        .withColumn('contact_name', F.trim(F.col('contact_name')) )\n", "        .withColumn('company_name', F.trim(F.col('company_name')) )\n", "        .withColumn('contact_title', F.trim(F.col('contact_title')) )\n", "        .withColumn('address', F.when(F.trim(F.col('address')) == '', None).otherwise(F.col('address')) )\n", "        .withColumn('city', F.trim(F.col('city')) )\n", "        .withColumn('country', F.trim(F.col('country')) )\n", "    )\n", "    df2 = df2.dropDuplicates(['customerid'])\n", "    selected = df2.select('customerid', 'company_name', 'contact_name', 'contact_title', 'address', 'city', 'region', 'postal_code', 'country', 'phone', 'fax')\n", "    return selected\n", "\n", "def products_silver(ingestion_date: str):\n", "    df = read_bronze('products', ingestion_date)\n", "    df = normalize_cols(df)\n", "    df2 = (df\n", "        .withColumn('unit_price', F.col('unit_price').cast(T.DoubleType()))\n", "        .withColumn('quantityperunit', F.trim(F.col('quantityperunit')) )\n", "        .withColumn('discontinued', F.col('discontinued').cast(T.IntegerType()))\n", "    )\n", "    try:\n", "        suppliers = read_bronze('suppliers', ingestion_date).select('supplierid', F.col('companyname').alias('supplier_name'))\n", "        df2 = df2.join(suppliers, on='supplierid', how='left')\n", "    except Exception:\n", "        pass\n", "    try:\n", "        categories = read_bronze('categories', ingestion_date).select('categoryid', F.col('categoryname').alias('category_name'))\n", "        df2 = df2.join(categories, on='categoryid', how='left')\n", "    except Exception:\n", "        pass\n", "    selected = df2.select('productid', 'product_name', 'supplierid', 'supplier_name', 'categoryid', 'category_name', 'quantityperunit', 'unit_price', 'unitsinstock', 'unitsonorder', 'reorderlevel', 'discontinued')\n", "    return selected\n", "\n", "def order_items_silver(ingestion_date: str):\n", "    df_od = read_bronze('order_details', ingestion_date)\n", "    try:\n", "        products = read_bronze('products', ingestion_date).select('productid', 'product_name', 'unit_price')\n", "        df2 = df_od.join(products, on='productid', how='left')\n", "    except Exception:\n", "        df2 = df_od\n", "    df2 = df2.withColumn('line_total', (F.col('unit_price') * F.col('quantity') * (1 - F.col('discount'))).cast(T.DoubleType()))\n", "    selected = df2.select('orderid', 'productid', 'unit_price', 'quantity', 'discount', 'line_total')\n", "    return selected\n", "\n", "def orders_silver(ingestion_date: str):\n", "    df_orders = read_bronze('orders', ingestion_date)\n", "    df_orders = normalize_cols(df_orders)\n", "    df2 = (df_orders\n", "        .withColumn('order_date', F.to_date(F.col('orderdate')) )\n", "        .withColumn('required_date', F.to_date(F.col('requireddate')) )\n", "        .withColumn('shipped_date', F.to_date(F.col('shippeddate')) )\n", "    )\n", "    try:\n", "        customers = read_bronze('customers', ingestion_date).select('customerid', F.col('company_name').alias('company_name'))\n", "        df2 = df2.join(customers, on='customerid', how='left')\n", "    except Exception:\n", "        pass\n", "    selected = df2.select('orderid', 'customerid', 'company_name', 'employeeid', 'order_date', 'required_date', 'shipped_date', 'freight', 'shipname', 'shipaddress', 'shipcity', 'shipregion', 'shippostalcode', 'shipcountry')\n", "    return selected\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Exécution des transformations (partition du jour)\n", "tables_and_funcs = [\n", "    ('customers', customers_silver),\n", "    ('products', products_silver),\n", "    ('order_items', order_items_silver),\n", "    ('orders', orders_silver),\n", "]\n", "results = []\n", "errors = []\n", "print('=== PHASE 2 - TRANSFORMATIONS SILVER ===')\n", "for name, func in tables_and_funcs:\n", "    try:\n", "        df_s = func(ingestion_date)\n", "        stats = write_silver(df_s, name, ingestion_date)\n", "        results.append(stats)\n", "        print(f\"[OK] {name} -> {stats['rows']} lignes -> {stats['path']}\")\n", "    except Exception as e:\n", "        errors.append({'table': name, 'error': str(e)})\n", "        print(f\"[ERREUR] {name} : {e}\")\n", "\n", "print('\nResume OK :')\n", "for r in results:\n", "    print(f\"  - {r['table']}: {r['rows']} lignes\")\n", "if errors:\n", "    print('\nResume erreurs :')\n", "    for err in errors:\n", "        print(f\"  - {err['table']}: {err['error']}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Vérification rapide: lire customers depuis Silver (après exécution)\n", "# Remarque: exécuter les cellules Phase 1 avant d'exécuter cette cellule\n", "df_customers_silver = spark.read.parquet(f\"s3a://silver/customers/{ingestion_date}\")\n", "df_customers_silver.select('_ingestion_timestamp', '_source_system', '_table_name', '_ingestion_date').show(5, truncate=False)\n", "df_customers_silver.printSchema()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.13.9"}}, "nbformat": 4, "nbformat_minor": 5}