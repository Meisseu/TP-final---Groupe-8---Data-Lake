{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847a4097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PySpark pip d√©j√† disponible - pyspark est importable\n",
      "‚úì Configuration termin√©e - PySpark pip utilisera son Spark embarqu√©\n",
      "‚úì Vous pouvez maintenant importer pyspark dans les cellules suivantes\n"
     ]
    }
   ],
   "source": [
    "# Configuration initiale pour JupyterLab Docker (n√©cessaire pour que pyspark soit importable)\n",
    "# Cette cellule doit √™tre ex√©cut√©e AVANT la cellule suivante\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# D√©tecter l'environnement Docker\n",
    "if os.path.exists('/usr/local/spark') or os.path.exists('/opt/spark'):\n",
    "    spark_home = os.environ.get('SPARK_HOME', '/usr/local/spark')\n",
    "    spark_python = os.path.join(spark_home, 'python')\n",
    "    \n",
    "    # Nettoyer TOUS les modules pyspark d√©j√† charg√©s\n",
    "    modules_to_remove = [k for k in list(sys.modules.keys()) if k.startswith('pyspark')]\n",
    "    for mod in modules_to_remove:\n",
    "        del sys.modules[mod]\n",
    "    \n",
    "    # Retirer le chemin syst√®me Spark de sys.path pour √©viter les conflits\n",
    "    if spark_python in sys.path:\n",
    "        sys.path.remove(spark_python)\n",
    "    \n",
    "    # Retirer SPARK_HOME de l'environnement pour que PySpark pip utilise son Spark embarqu√©\n",
    "    if 'SPARK_HOME' in os.environ:\n",
    "        del os.environ['SPARK_HOME']\n",
    "    \n",
    "    # V√©rifier si pyspark.sql.SparkSession est importable\n",
    "    try:\n",
    "        # Tester l'import direct (avec PySpark pip si install√©)\n",
    "        from pyspark.sql import SparkSession\n",
    "        print(\"‚úì PySpark pip d√©j√† disponible - pyspark est importable\")\n",
    "    except (ImportError, AttributeError):\n",
    "        # Installer PySpark via pip\n",
    "        print(\"‚ö† Installation de PySpark via pip...\")\n",
    "        import subprocess\n",
    "        # Installer PySpark compatible avec Spark 4.0.1\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyspark==3.5.0\", \"-q\"])\n",
    "        # Nettoyer √† nouveau les modules\n",
    "        modules_to_remove = [k for k in list(sys.modules.keys()) if k.startswith('pyspark')]\n",
    "        for mod in modules_to_remove:\n",
    "            del sys.modules[mod]\n",
    "        # Tester l'import apr√®s installation\n",
    "        from pyspark.sql import SparkSession\n",
    "        print(\"‚úì PySpark install√© via pip - pyspark est maintenant importable\")\n",
    "    \n",
    "    # S'assurer que le chemin pip est prioritaire\n",
    "    import site\n",
    "    site_packages = [p for p in sys.path if 'site-packages' in p]\n",
    "    if site_packages:\n",
    "        # D√©placer site-packages en d√©but de sys.path\n",
    "        for sp in site_packages:\n",
    "            if sp in sys.path:\n",
    "                sys.path.remove(sp)\n",
    "                sys.path.insert(0, sp)\n",
    "    \n",
    "    # IMPORTANT: S'assurer que PySpark pip utilise son propre Spark embarqu√©\n",
    "    # Ne pas d√©finir SPARK_HOME pour que PySpark pip utilise son Spark int√©gr√©\n",
    "    if 'SPARK_HOME' in os.environ:\n",
    "        del os.environ['SPARK_HOME']\n",
    "    \n",
    "    print(\"‚úì Configuration termin√©e - PySpark pip utilisera son Spark embarqu√©\")\n",
    "    print(\"‚úì Vous pouvez maintenant importer pyspark dans les cellules suivantes\")\n",
    "else:\n",
    "    print(\"Environnement local d√©tect√© - utilisation de PySpark pip si disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958acdab",
   "metadata": {},
   "source": [
    "## 1) Contexte d'ex√©cution\n",
    "Ce notebook est fait pour √™tre ex√©cut√© **dans le JupyterLab de l'environnement Docker** (service `jupyter-spark`).\n",
    "\n",
    "- PostgreSQL dans Docker : `jdbc:postgresql://postgres:5432/app`\n",
    "- MinIO dans Docker : `http://minio:9000`\n",
    "- Buckets attendus : `bronze`, `silver`, `gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d58b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c9d492c5-48c9-4d10-8051-96a25875399b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 204ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c9d492c5-48c9-4d10-8051-96a25875399b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n",
      "26/01/16 15:56:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK\n",
      "S3A endpoint = http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# SparkSession avec config MinIO + drivers (au besoin)\n",
    "# NOTE: Utilisation de hadoop-aws:3.3.4 au lieu de 3.4.1 pour √©viter l'erreur BulkDelete\n",
    "# hadoop-aws:3.3.4 est compatible avec PySpark 3.5.0 et ne n√©cessite pas BulkDelete\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"TP Final - Phase 1 Bronze\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Pour permettre overwrite uniquement de la partition du jour\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "print(\"Spark OK\")\n",
    "print(\"S3A endpoint =\", spark.sparkContext._jsc.hadoopConfiguration().get(\"fs.s3a.endpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc3233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion date = 2026-01-16\n"
     ]
    }
   ],
   "source": [
    "# Configuration JDBC PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/app\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "# Date d‚Äôingestion (partition) au format YYYY-MM-DD\n",
    "ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(\"Ingestion date =\", ingestion_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4330fb",
   "metadata": {},
   "source": [
    "## 2) Fonction d‚Äôingestion generique (PostgreSQL ‚Üí Bronze)\n",
    "Cette fonction :\n",
    "- lit la table via JDBC\n",
    "- ajoute les colonnes techniques obligatoires\n",
    "- ecrit en Parquet dans `s3a://bronze/<table_name>/<YYYY-MM-DD>/` (partitionnement par dossier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b4cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_table_to_bronze(table_name: str, *, ingestion_date: str, base_path: str = \"s3a://bronze\") -> dict:\n",
    "    \"\"\"\n",
    "    Ingerer une table PostgreSQL vers la zone Bronze sur MinIO.\n",
    "\n",
    "    Ecrit en Parquet et partitionne par date d‚Äôingestion (YYYY-MM-DD) via la structure de dossiers.\n",
    "    Ajoute les metadonnees techniques :\n",
    "      - _ingestion_timestamp\n",
    "      - _source_system = 'postgresql'\n",
    "      - _table_name\n",
    "      - _ingestion_date\n",
    "\n",
    "    Retourne un petit dictionnaire de stats.\n",
    "    \"\"\"\n",
    "    ingestion_ts_col = F.current_timestamp()\n",
    "\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n",
    "    row_count = df.count()\n",
    "\n",
    "    df_out = (df\n",
    "        .withColumn(\"_ingestion_timestamp\", ingestion_ts_col)\n",
    "        .withColumn(\"_source_system\", F.lit(\"postgresql\"))\n",
    "        .withColumn(\"_table_name\", F.lit(table_name))\n",
    "        .withColumn(\"_ingestion_date\", F.lit(ingestion_date))\n",
    "    )\n",
    "\n",
    "    target_path = f\"{base_path}/{table_name}/{ingestion_date}\"\n",
    "\n",
    "    df_out.write.mode(\"overwrite\").parquet(target_path)\n",
    "\n",
    "    return {\n",
    "        \"table\": table_name,\n",
    "        \"rows\": row_count,\n",
    "        \"path\": target_path,\n",
    "        \"partition\": ingestion_date,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fcc38",
   "metadata": {},
   "source": [
    "## 3) Execution Phase 1 : tables obligatoires (+ bonus)\n",
    "Tables obligatoires : `customers`, `orders`, `order_details`, `products`\n",
    "\n",
    "Tables bonus (si dispo) : `employees`, `suppliers`, `categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3892084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1 - INGESTION BRONZE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/16 15:56:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] customers -> 91 lignes -> s3a://bronze/customers/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] orders -> 830 lignes -> s3a://bronze/orders/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] order_details -> 2155 lignes -> s3a://bronze/order_details/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] products -> 77 lignes -> s3a://bronze/products/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] employees -> 9 lignes -> s3a://bronze/employees/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] suppliers -> 29 lignes -> s3a://bronze/suppliers/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] categories -> 8 lignes -> s3a://bronze/categories/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "\n",
      "Resume OK :\n",
      "  - customers: 91 lignes\n",
      "  - orders: 830 lignes\n",
      "  - order_details: 2155 lignes\n",
      "  - products: 77 lignes\n",
      "  - employees: 9 lignes\n",
      "  - suppliers: 29 lignes\n",
      "  - categories: 8 lignes\n"
     ]
    }
   ],
   "source": [
    "required_tables = [\"customers\", \"orders\", \"order_details\", \"products\"]\n",
    "bonus_tables = [\"employees\", \"suppliers\", \"categories\"]\n",
    "tables_to_ingest = required_tables + bonus_tables\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "print(\"=== PHASE 1 - INGESTION BRONZE ===\")\n",
    "for table in tables_to_ingest:\n",
    "    try:\n",
    "        stats = ingest_table_to_bronze(table, ingestion_date=ingestion_date)\n",
    "        results.append(stats)\n",
    "        print(f\"[OK] {stats['table']} -> {stats['rows']} lignes -> {stats['path']} (_ingestion_date={stats['partition']})\")\n",
    "    except Exception as e:\n",
    "        errors.append({\"table\": table, \"error\": str(e)})\n",
    "        print(f\"[ERREUR] {table} : {e}\")\n",
    "\n",
    "print(\"\\nResume OK :\")\n",
    "for r in results:\n",
    "    print(f\"  - {r['table']}: {r['rows']} lignes\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nResume erreurs :\")\n",
    "    for err in errors:\n",
    "        print(f\"  - {err['table']}: {err['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329dd24",
   "metadata": {},
   "source": [
    "## 4) Verifications rapides\n",
    "On relit une table depuis Bronze et on verifie la presence des colonnes techniques + la partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f623ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes manquantes : set()\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "|_ingestion_timestamp      |_source_system|_table_name|_ingestion_date|\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "|2026-01-16 15:56:39.417705|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 15:56:39.417705|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 15:56:39.417705|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 15:56:39.417705|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 15:56:39.417705|postgresql    |customers  |2026-01-16     |\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|_ingestion_date|count|\n",
      "+---------------+-----+\n",
      "|2026-01-16     |91   |\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple : verifier customers (lecture de la partition du jour)\n",
    "df_bronze_customers = spark.read.parquet(f\"s3a://bronze/customers/{ingestion_date}\")\n",
    "\n",
    "expected_cols = {\"_ingestion_timestamp\", \"_source_system\", \"_table_name\", \"_ingestion_date\"}\n",
    "missing = expected_cols - set(df_bronze_customers.columns)\n",
    "print(\"Colonnes manquantes :\", missing)\n",
    "\n",
    "df_bronze_customers.select(\n",
    "    \"_ingestion_timestamp\",\n",
    "    \"_source_system\",\n",
    "    \"_table_name\",\n",
    "    \"_ingestion_date\",\n",
    ").show(5, truncate=False)\n",
    "\n",
    "df_bronze_customers.groupBy(\"_ingestion_date\").count().orderBy(\"_ingestion_date\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc3401",
   "metadata": {},
   "source": [
    "# TP Final ‚Äî Phase 2 : Raffinement Silver (Bronze ‚Üí Silver)\n",
    "\n",
    "Objectif : nettoyer, typer et structurer les donnees Bronze pour les rendre exploitables en Silver.\n",
    "\n",
    "Sorties attendues (minimum) :\n",
    "- `dim_customers` : `company_name` en InitCap, `country` en MAJUSCULES\n",
    "- `dim_products` : jointure produits ‚Üî categories + `stock_status` (CRITIQUE si stock < 10, sinon NORMAL)\n",
    "- `fact_orders` : jointure `orders` + `order_details` + calcul `montant_net`\n",
    "\n",
    "Ecriture en Parquet sous `s3a://silver/<dataset>/<YYYY-MM-DD>/` (meme partition jour que la Phase 1).\n",
    "\n",
    "> Important : cette Phase 2 lit la partition Bronze du jour `s3a://bronze/<table>/<YYYY-MM-DD>/` produite par la Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a62e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def _first_existing_column(df, candidates: list[str]) -> str:\n",
    "    for name in candidates:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    raise ValueError(f\"Aucune colonne trouvee parmi {candidates}. Colonnes dispo: {df.columns}\")\n",
    "\n",
    "def _canonicalize_columns(df, mapping: dict[str, list[str]]):\n",
    "    # mapping: canonical_name -> [candidates...]\n",
    "    out = df\n",
    "    for canonical, candidates in mapping.items():\n",
    "        found = _first_existing_column(out, candidates)\n",
    "        if found != canonical:\n",
    "            out = out.withColumnRenamed(found, canonical)\n",
    "    return out\n",
    "\n",
    "def read_bronze(table_name: str, ingestion_date: str):\n",
    "    path = f\"s3a://bronze/{table_name}/{ingestion_date}\"\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def write_silver(df, dataset_name: str, ingestion_date: str, base_path: str = \"s3a://silver\") -> dict:\n",
    "    target_path = f\"{base_path}/{dataset_name}/{ingestion_date}\"\n",
    "    row_count = df.count()\n",
    "    df.write.mode(\"overwrite\").parquet(target_path)\n",
    "    return {\"dataset\": dataset_name, \"rows\": row_count, \"path\": target_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b663ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 2 - ECRITURE SILVER OK ===\n",
      "[OK] dim_customers -> 91 lignes -> s3a://silver/dim_customers/2026-01-16\n",
      "[OK] dim_products -> 77 lignes -> s3a://silver/dim_products/2026-01-16\n",
      "[OK] fact_orders -> 2155 lignes -> s3a://silver/fact_orders/2026-01-16\n"
     ]
    }
   ],
   "source": [
    "# --- Lecture Bronze (partition du jour) ---\n",
    "customers_bronze = read_bronze(\"customers\", ingestion_date)\n",
    "orders_bronze = read_bronze(\"orders\", ingestion_date)\n",
    "order_details_bronze = read_bronze(\"order_details\", ingestion_date)\n",
    "products_bronze = read_bronze(\"products\", ingestion_date)\n",
    "\n",
    "# Tables bonus (si presentes) : categories (pour dim_products)\n",
    "categories_bronze = None\n",
    "try:\n",
    "    categories_bronze = read_bronze(\"categories\", ingestion_date)\n",
    "except Exception as e:\n",
    "    print(\"[INFO] categories non disponible en Bronze:\", e)\n",
    "\n",
    "# --- 2.1 Dim_Customers ---\n",
    "# R√®gles : company_name InitCap, country UPPER + dedup cl√© m√©tier\n",
    "customers = _canonicalize_columns(customers_bronze, {\n",
    "    \"customer_id\": [\"customer_id\", \"customerid\", \"CustomerID\"],\n",
    "    \"company_name\": [\"company_name\", \"companyname\", \"CompanyName\"],\n",
    "    \"country\": [\"country\", \"Country\"],\n",
    "})\n",
    "dim_customers = (customers\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "    .withColumn(\"company_name\", F.initcap(F.col(\"company_name\")))\n",
    "    .withColumn(\"country\", F.upper(F.col(\"country\")))\n",
    "    .select(\"customer_id\", \"company_name\", \"country\")\n",
    " )\n",
    "\n",
    "# --- 2.2 Dim_Products ---\n",
    "# R√®gles : join produits ‚Üî categories + stock_status\n",
    "products = _canonicalize_columns(products_bronze, {\n",
    "    \"product_id\": [\"product_id\", \"productid\", \"ProductID\"],\n",
    "    \"product_name\": [\"product_name\", \"productname\", \"ProductName\"],\n",
    "    \"category_id\": [\"category_id\", \"categoryid\", \"CategoryID\"],\n",
    "    \"units_in_stock\": [\"units_in_stock\", \"unitsinstock\", \"UnitsInStock\"],\n",
    "})\n",
    "products = (products\n",
    "    .withColumn(\"units_in_stock\", F.col(\"units_in_stock\").cast(\"int\"))\n",
    "    .dropDuplicates([\"product_id\"])\n",
    " )\n",
    "if categories_bronze is not None:\n",
    "    categories = _canonicalize_columns(categories_bronze, {\n",
    "        \"category_id\": [\"category_id\", \"categoryid\", \"CategoryID\"],\n",
    "        \"category_name\": [\"category_name\", \"categoryname\", \"CategoryName\"],\n",
    "    }).dropDuplicates([\"category_id\"])\n",
    "    products = products.join(categories.select(\"category_id\", \"category_name\"), on=\"category_id\", how=\"left\")\n",
    "else:\n",
    "    products = products.withColumn(\"category_name\", F.lit(None).cast(\"string\"))\n",
    "dim_products = (products\n",
    "    .withColumn(\"stock_status\", F.when(F.col(\"units_in_stock\") < 10, F.lit(\"CRITIQUE\")).otherwise(F.lit(\"NORMAL\")))\n",
    "    .select(\"product_id\", \"product_name\", \"category_id\", \"category_name\", \"units_in_stock\", \"stock_status\")\n",
    " )\n",
    "\n",
    "# --- 2.3 Fact_Orders ---\n",
    "# R√®gles : jointure orders + order_details + montant_net\n",
    "orders = _canonicalize_columns(orders_bronze, {\n",
    "    \"order_id\": [\"order_id\", \"orderid\", \"OrderID\"],\n",
    "    \"customer_id\": [\"customer_id\", \"customerid\", \"CustomerID\"],\n",
    "    \"order_date\": [\"order_date\", \"orderdate\", \"OrderDate\"],\n",
    "})\n",
    "order_details = _canonicalize_columns(order_details_bronze, {\n",
    "    \"order_id\": [\"order_id\", \"orderid\", \"OrderID\"],\n",
    "    \"product_id\": [\"product_id\", \"productid\", \"ProductID\"],\n",
    "    \"unit_price\": [\"unit_price\", \"unitprice\", \"UnitPrice\"],\n",
    "    \"quantity\": [\"quantity\", \"Quantity\"],\n",
    "    \"discount\": [\"discount\", \"Discount\"],\n",
    "})\n",
    "orders = orders.withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "order_details = (order_details\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"discount\", F.col(\"discount\").cast(\"double\"))\n",
    "    .dropDuplicates([\"order_id\", \"product_id\"])\n",
    " )\n",
    "fact_orders = (orders.select(\"order_id\", \"customer_id\", \"order_date\")\n",
    "    .join(order_details, on=\"order_id\", how=\"inner\")\n",
    "    .withColumn(\"montant_net\", (F.col(\"unit_price\") * F.col(\"quantity\") * (F.lit(1.0) - F.col(\"discount\"))).cast(\"double\"))\n",
    "    .select(\"order_id\", \"product_id\", \"customer_id\", \"order_date\", \"quantity\", \"unit_price\", \"discount\", \"montant_net\")\n",
    " )\n",
    "\n",
    "# --- Ecriture Silver ---\n",
    "results_silver = [\n",
    "    write_silver(dim_customers, \"dim_customers\", ingestion_date),\n",
    "    write_silver(dim_products, \"dim_products\", ingestion_date),\n",
    "    write_silver(fact_orders, \"fact_orders\", ingestion_date),\n",
    "]\n",
    "print(\"=== PHASE 2 - ECRITURE SILVER OK ===\")\n",
    "for r in results_silver:\n",
    "    print(f\"[OK] {r['dataset']} -> {r['rows']} lignes -> {r['path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25bc0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------+-------+\n",
      "|customer_id|company_name                      |country|\n",
      "+-----------+----------------------------------+-------+\n",
      "|ALFKI      |Alfreds Futterkiste               |GERMANY|\n",
      "|ANATR      |Ana Trujillo Emparedados Y Helados|MEXICO |\n",
      "|ANTON      |Antonio Moreno Taquer√≠a           |MEXICO |\n",
      "|AROUT      |Around The Horn                   |UK     |\n",
      "|BERGS      |Berglunds Snabbk√∂p                |SWEDEN |\n",
      "+-----------+----------------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification rapide : relire une sortie Silver\n",
    "df_check_dim_customers = spark.read.parquet(f\"s3a://silver/dim_customers/{ingestion_date}\")\n",
    "df_check_dim_customers.show(5, truncate=False)\n",
    "df_check_dim_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7659f",
   "metadata": {},
   "source": [
    "## 3.1 - Script Python Producer Kafka\n",
    "\n",
    "Simulation de nouvelles commandes envoy√©es en temps r√©el sur le topic `telemetry_orders`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef29c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì kafka-python d√©j√† install√©\n",
      "Producer Kafka configur√© pour le topic: telemetry_orders\n",
      "Broker: broker:29092\n"
     ]
    }
   ],
   "source": [
    "# Installation de kafka-python si n√©cessaire\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from kafka import KafkaProducer\n",
    "    print(\"‚úì kafka-python d√©j√† install√©\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† Installation de kafka-python...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kafka-python\", \"-q\"])\n",
    "    from kafka import KafkaProducer\n",
    "    print(\"‚úì kafka-python install√© avec succ√®s\")\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "# Configuration Kafka\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "TOPIC = \"telemetry_orders\"\n",
    "\n",
    "# Initialisation du producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[KAFKA_BROKER],\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "print(f\"Producer Kafka configur√© pour le topic: {TOPIC}\")\n",
    "print(f\"Broker: {KAFKA_BROKER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c132985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G√©n√©ration de messages de test...\n",
      "Message 1 envoy√©: {\n",
      "  \"order_id\": 83767,\n",
      "  \"customer_id\": \"BOLID\",\n",
      "  \"product_id\": 43,\n",
      "  \"quantity\": 37,\n",
      "  \"unit_price\": 71.67,\n",
      "  \"discount\": 0.13,\n",
      "  \"status\": \"pending\",\n",
      "  \"order_timestamp\": \"2026-01-16T16:00:42.872661+00:00\"\n",
      "}\n",
      "Message 2 envoy√©: {\n",
      "  \"order_id\": 17447,\n",
      "  \"customer_id\": \"BOTTM\",\n",
      "  \"product_id\": 6,\n",
      "  \"quantity\": 26,\n",
      "  \"unit_price\": 322.98,\n",
      "  \"discount\": 0.25,\n",
      "  \"status\": \"delivered\",\n",
      "  \"order_timestamp\": \"2026-01-16T16:00:43.376755+00:00\"\n",
      "}\n",
      "Message 3 envoy√©: {\n",
      "  \"order_id\": 37081,\n",
      "  \"customer_id\": \"BOLID\",\n",
      "  \"product_id\": 65,\n",
      "  \"quantity\": 1,\n",
      "  \"unit_price\": 479.27,\n",
      "  \"discount\": 0.27,\n",
      "  \"status\": \"processing\",\n",
      "  \"order_timestamp\": \"2026-01-16T16:00:43.877241+00:00\"\n",
      "}\n",
      "Message 4 envoy√©: {\n",
      "  \"order_id\": 44216,\n",
      "  \"customer_id\": \"BONAP\",\n",
      "  \"product_id\": 41,\n",
      "  \"quantity\": 18,\n",
      "  \"unit_price\": 266.65,\n",
      "  \"discount\": 0.14,\n",
      "  \"status\": \"shipped\",\n",
      "  \"order_timestamp\": \"2026-01-16T16:00:44.377726+00:00\"\n",
      "}\n",
      "Message 5 envoy√©: {\n",
      "  \"order_id\": 66510,\n",
      "  \"customer_id\": \"BONAP\",\n",
      "  \"product_id\": 5,\n",
      "  \"quantity\": 36,\n",
      "  \"unit_price\": 353.38,\n",
      "  \"discount\": 0.06,\n",
      "  \"status\": \"confirmed\",\n",
      "  \"order_timestamp\": \"2026-01-16T16:00:44.878239+00:00\"\n",
      "}\n",
      "\n",
      "5 messages envoy√©s sur le topic telemetry_orders\n"
     ]
    }
   ],
   "source": [
    "# Donn√©es de r√©f√©rence pour g√©n√©rer des commandes r√©alistes\n",
    "CUSTOMER_IDS = [\"ALFKI\", \"ANATR\", \"ANTON\", \"AROUT\", \"BERGS\", \"BLAUS\", \"BLONP\", \"BOLID\", \"BONAP\", \"BOTTM\"]\n",
    "PRODUCT_IDS = list(range(1, 78))  # IDs produits de 1 √† 77\n",
    "STATUSES = [\"pending\", \"confirmed\", \"processing\", \"shipped\", \"delivered\"]\n",
    "\n",
    "def generate_order_message():\n",
    "    \"\"\"G√©n√®re un message JSON simulant une nouvelle commande.\"\"\"\n",
    "    order_id = random.randint(10000, 99999)\n",
    "    customer_id = random.choice(CUSTOMER_IDS)\n",
    "    product_id = random.choice(PRODUCT_IDS)\n",
    "    quantity = random.randint(1, 50)\n",
    "    unit_price = round(random.uniform(10.0, 500.0), 2)\n",
    "    discount = round(random.uniform(0.0, 0.3), 2)\n",
    "    status = random.choice(STATUSES)\n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    message = {\n",
    "        \"order_id\": order_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"product_id\": product_id,\n",
    "        \"quantity\": quantity,\n",
    "        \"unit_price\": unit_price,\n",
    "        \"discount\": discount,\n",
    "        \"status\": status,\n",
    "        \"order_timestamp\": timestamp\n",
    "    }\n",
    "    return message\n",
    "\n",
    "# Test : g√©n√©rer et envoyer quelques messages\n",
    "print(\"G√©n√©ration de messages de test...\")\n",
    "for i in range(5):\n",
    "    msg = generate_order_message()\n",
    "    producer.send(TOPIC, msg)\n",
    "    print(f\"Message {i+1} envoy√©: {json.dumps(msg, indent=2)}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "producer.flush()\n",
    "print(f\"\\n{5} messages envoy√©s sur le topic {TOPIC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70d3f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports et v√©rifications OK\n"
     ]
    }
   ],
   "source": [
    "# Imports n√©cessaires pour la g√©n√©ration de messages\n",
    "# Cette cellule doit √™tre ex√©cut√©e avant la cellule suivante si vous ex√©cutez les cellules individuellement\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "# V√©rifier que producer et TOPIC sont d√©finis\n",
    "if 'producer' not in globals():\n",
    "    print(\"[ERREUR] Variable 'producer' non trouv√©e.\")\n",
    "    print(\"‚ö† Veuillez d'abord ex√©cuter la cellule 16 pour configurer le producer Kafka.\")\n",
    "    raise NameError(\"Variable 'producer' non trouv√©e\")\n",
    "\n",
    "if 'TOPIC' not in globals():\n",
    "    print(\"[ERREUR] Variable 'TOPIC' non trouv√©e.\")\n",
    "    print(\"‚ö† Veuillez d'abord ex√©cuter la cellule 16 pour configurer le topic Kafka.\")\n",
    "    raise NameError(\"Variable 'TOPIC' non trouv√©e\")\n",
    "\n",
    "print(\"‚úì Imports et v√©rifications OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31429492",
   "metadata": {},
   "source": [
    "## 3.2 - Job Spark Structured Streaming\n",
    "\n",
    "Lecture des messages Kafka, parsing JSON et √©criture dans Bronze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eab8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connecteur Kafka disponible\n",
      "‚úì Vous pouvez maintenant ex√©cuter la cellule suivante (lecture du stream Kafka)\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC ET CORRECTION : V√©rifier si le connecteur Kafka est disponible\n",
    "# Si ce n'est pas le cas, cette cellule va arr√™ter et recr√©er la session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import time as time_module\n",
    "\n",
    "def check_kafka_connector():\n",
    "    \"\"\"V√©rifie si le connecteur Kafka est disponible.\"\"\"\n",
    "    try:\n",
    "        if 'spark_streaming' not in globals():\n",
    "            return False, \"spark_streaming n'existe pas\"\n",
    "        \n",
    "        # Tester si le format kafka est disponible\n",
    "        test_reader = spark_streaming.readStream.format(\"kafka\")\n",
    "        return True, \"Connecteur Kafka disponible\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# V√©rifier le connecteur\n",
    "is_available, message = check_kafka_connector()\n",
    "\n",
    "if not is_available:\n",
    "    print(f\"‚ùå Connecteur Kafka non disponible: {message}\")\n",
    "    print(\"\\nüîß CORRECTION AUTOMATIQUE: Arr√™t et recr√©ation de la session...\")\n",
    "    \n",
    "    # Arr√™ter toutes les sessions\n",
    "    try:\n",
    "        if 'spark_streaming' in globals():\n",
    "            spark_streaming.stop()\n",
    "            del spark_streaming\n",
    "        active = SparkSession.getActiveSession()\n",
    "        if active and \"Kafka\" in active.sparkContext.appName:\n",
    "            active.stop()\n",
    "        time_module.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Recr√©er la session avec un nom unique\n",
    "    app_name_unique = f\"TP-Final-Phase3-Kafka-{int(time_module.time())}\"\n",
    "    print(f\"   Cr√©ation d'une nouvelle session: {app_name_unique}\")\n",
    "    print(\"   ‚è≥ T√©l√©chargement des packages (20-40 secondes)...\")\n",
    "    \n",
    "    spark_streaming = (SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(app_name_unique)\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "                \"org.apache.kafka:kafka-clients:3.5.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "        .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .getOrCreate())\n",
    "    \n",
    "    spark_streaming.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    # Attendre le t√©l√©chargement\n",
    "    time_module.sleep(10)\n",
    "    \n",
    "    # V√©rifier √† nouveau\n",
    "    is_available, message = check_kafka_connector()\n",
    "    if is_available:\n",
    "        print(\"‚úì Connecteur Kafka maintenant disponible !\")\n",
    "    else:\n",
    "        print(f\"‚ö† Connecteur toujours non disponible: {message}\")\n",
    "        print(\"   Veuillez attendre 20-30 secondes suppl√©mentaires et r√©ex√©cuter cette cellule.\")\n",
    "else:\n",
    "    print(f\"‚úì {message}\")\n",
    "    print(\"‚úì Vous pouvez maintenant ex√©cuter la cellule suivante (lecture du stream Kafka)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a24bac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Arr√™t de la SparkSession existante pour recharger les packages Kafka...\n",
      "‚úì SparkSession cr√©√©e\n",
      "‚è≥ Les packages Kafka sont en cours de t√©l√©chargement (premi√®re ex√©cution uniquement)...\n",
      "   Cela peut prendre 10-30 secondes. Veuillez patienter.\n",
      "\n",
      "‚úì Connecteur Kafka disponible et pr√™t\n",
      "\n",
      "Spark Streaming configur√©\n",
      "Kafka broker: broker:29092\n",
      "Topic: telemetry_orders\n",
      "Checkpoint: s3a://bronze/checkpoints/kafka_orders\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# SparkSession avec support Kafka\n",
    "# NOTE: Utilisation de hadoop-aws:3.3.4 au lieu de 3.4.1 pour √©viter l'erreur BulkDelete\n",
    "# IMPORTANT: Arr√™ter toute session existante pour forcer le chargement des packages Kafka\n",
    "try:\n",
    "    existing_spark = SparkSession.getActiveSession()\n",
    "    if existing_spark:\n",
    "        app_name = existing_spark.sparkContext.appName\n",
    "        if \"Phase 3 Streaming Kafka\" in app_name or \"spark_streaming\" in globals():\n",
    "            print(\"[INFO] Arr√™t de la SparkSession existante pour recharger les packages Kafka...\")\n",
    "            existing_spark.stop()\n",
    "            # Nettoyer la variable globale si elle existe\n",
    "            if 'spark_streaming' in globals():\n",
    "                del spark_streaming\n",
    "            import time\n",
    "            time.sleep(2)  # Attendre que la session soit compl√®tement arr√™t√©e\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] Aucune session existante √† arr√™ter: {e}\")\n",
    "\n",
    "# Cr√©er une nouvelle SparkSession avec les packages Kafka\n",
    "# NOTE: Utilisation de .master(\"local[*]\") pour forcer une nouvelle session\n",
    "# Les packages seront t√©l√©charg√©s automatiquement au premier d√©marrage (peut prendre 10-30 secondes)\n",
    "spark_streaming = (SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"TP Final - Phase 3 Streaming Kafka\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"org.apache.kafka:kafka-clients:3.5.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark_streaming.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úì SparkSession cr√©√©e\")\n",
    "print(\"‚è≥ Les packages Kafka sont en cours de t√©l√©chargement (premi√®re ex√©cution uniquement)...\")\n",
    "print(\"   Cela peut prendre 10-30 secondes. Veuillez patienter.\\n\")\n",
    "\n",
    "# Attendre un peu pour que les packages soient t√©l√©charg√©s\n",
    "import time\n",
    "time.sleep(5)  # Attendre 5 secondes pour le t√©l√©chargement initial\n",
    "\n",
    "# V√©rifier que le connecteur Kafka est disponible\n",
    "try:\n",
    "    # Tester si le format kafka est disponible\n",
    "    test_df = spark_streaming.readStream.format(\"kafka\")\n",
    "    print(\"‚úì Connecteur Kafka disponible et pr√™t\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† ERREUR: Connecteur Kafka non disponible: {e}\")\n",
    "    print(\"‚ö† Les packages peuvent prendre plus de temps √† t√©l√©charger.\")\n",
    "    print(\"‚ö† SOLUTION: R√©ex√©cutez cette cellule apr√®s 10-20 secondes.\")\n",
    "    print(\"   Les packages seront t√©l√©charg√©s au premier d√©marrage et mis en cache.\")\n",
    "    raise\n",
    "\n",
    "KAFKA_BROKER = \"broker:29092\"\n",
    "KAFKA_TOPIC = \"telemetry_orders\"\n",
    "BRONZE_PATH = \"s3a://bronze/kafka_orders\"\n",
    "CHECKPOINT_LOCATION = \"s3a://bronze/checkpoints/kafka_orders\"\n",
    "\n",
    "print(\"\\nSpark Streaming configur√©\")\n",
    "print(f\"Kafka broker: {KAFKA_BROKER}\")\n",
    "print(f\"Topic: {KAFKA_TOPIC}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_LOCATION}\")\n",
    "print(f\"Spark version: {spark_streaming.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a497284",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      2\u001b[39m order_schema = StructType([\n\u001b[32m      3\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      4\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33morder_timestamp\u001b[39m\u001b[33m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     11\u001b[39m ])\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Lecture du stream Kafka\u001b[39;00m\n\u001b[32m     14\u001b[39m df_kafka_stream = (\u001b[43mspark_streaming\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadStream\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka.bootstrap.servers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAFKA_BROKER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubscribe\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAFKA_TOPIC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstartingOffsets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mearliest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStream Kafka configur√©\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/pyspark/sql/streaming/readwriter.py:304\u001b[39m, in \u001b[36mDataStreamReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(path))\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "# Sch√©ma JSON des messages de commandes\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Lecture du stream Kafka\n",
    "df_kafka_stream = (spark_streaming.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load())\n",
    "\n",
    "print(\"Stream Kafka configur√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f63c2965",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_kafka_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Parsing JSON et ajout des m√©tadonn√©es techniques\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_parsed = (\u001b[43mdf_kafka_stream\u001b[49m\n\u001b[32m      3\u001b[39m     .select(\n\u001b[32m      4\u001b[39m         F.from_json(F.col(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m), order_schema).alias(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mkafka_timestamp\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mpartition\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33moffset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     .select(\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata.*\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         F.current_timestamp().alias(\u001b[33m\"\u001b[39m\u001b[33m_ingestion_timestamp\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m         F.lit(\u001b[33m\"\u001b[39m\u001b[33mkafka\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33m_source_system\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     13\u001b[39m         F.lit(\u001b[33m\"\u001b[39m\u001b[33mtelemetry_orders\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33m_table_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     14\u001b[39m         F.to_date(F.current_timestamp()).alias(\u001b[33m\"\u001b[39m\u001b[33m_ingestion_date\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkafka_timestamp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpartition\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moffset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33morder_timestamp\u001b[39m\u001b[33m\"\u001b[39m, F.to_timestamp(F.col(\u001b[33m\"\u001b[39m\u001b[33morder_timestamp\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mParsing JSON et ajout des m√©tadonn√©es techniques effectu√©s\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_kafka_stream' is not defined"
     ]
    }
   ],
   "source": [
    "# Parsing JSON et ajout des m√©tadonn√©es techniques\n",
    "df_parsed = (df_kafka_stream\n",
    "    .select(\n",
    "        F.from_json(F.col(\"value\").cast(\"string\"), order_schema).alias(\"data\"),\n",
    "        F.col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
    "        F.col(\"partition\"),\n",
    "        F.col(\"offset\")\n",
    "    )\n",
    "    .select(\n",
    "        \"data.*\",\n",
    "        F.current_timestamp().alias(\"_ingestion_timestamp\"),\n",
    "        F.lit(\"kafka\").alias(\"_source_system\"),\n",
    "        F.lit(\"telemetry_orders\").alias(\"_table_name\"),\n",
    "        F.to_date(F.current_timestamp()).alias(\"_ingestion_date\"),\n",
    "        \"kafka_timestamp\",\n",
    "        \"partition\",\n",
    "        \"offset\"\n",
    "    )\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_timestamp\")))\n",
    ")\n",
    "\n",
    "print(\"Parsing JSON et ajout des m√©tadonn√©es techniques effectu√©s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f1312f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_parsed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# √âcriture en streaming vers Bronze avec append mode et checkpoint\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m query = (\u001b[43mdf_parsed\u001b[49m.writeStream\n\u001b[32m      3\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m, BRONZE_PATH)\n\u001b[32m      6\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mcheckpointLocation\u001b[39m\u001b[33m\"\u001b[39m, CHECKPOINT_LOCATION)\n\u001b[32m      7\u001b[39m     .trigger(processingTime=\u001b[33m\"\u001b[39m\u001b[33m10 seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     .start())\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== PHASE 3 - STREAMING KAFKA D√âMARR√â ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m√âcriture dans: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBRONZE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_parsed' is not defined"
     ]
    }
   ],
   "source": [
    "# √âcriture en streaming vers Bronze avec append mode et checkpoint\n",
    "query = (df_parsed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", BRONZE_PATH)\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_LOCATION)\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .start())\n",
    "\n",
    "print(\"=== PHASE 3 - STREAMING KAFKA D√âMARR√â ===\")\n",
    "print(f\"√âcriture dans: {BRONZE_PATH}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_LOCATION}\")\n",
    "print(f\"Mode: append\")\n",
    "print(\"\\nLe streaming est actif. Envoyez des messages sur le topic pour les voir appara√Ætre dans Bronze.\")\n",
    "print(\"Pour arr√™ter le streaming, ex√©cutez: query.stop()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41c959bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification : lecture des donn√©es √©crites en Bronze (apr√®s quelques secondes de streaming)\n",
    "# D√©commentez cette cellule apr√®s avoir laiss√© le streaming tourner quelques secondes\n",
    "\n",
    "# df_bronze_kafka = spark_streaming.read.parquet(BRONZE_PATH)\n",
    "# print(f\"Nombre de messages ing√©r√©s: {df_bronze_kafka.count()}\")\n",
    "# df_bronze_kafka.show(10, truncate=False)\n",
    "# df_bronze_kafka.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba5b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Aucun streaming actif trouv√© (variable 'query' non d√©finie)\n"
     ]
    }
   ],
   "source": [
    "# Arr√™ter le streaming Kafka\n",
    "# Ex√©cutez cette cellule quand vous avez termin√© de tester le streaming\n",
    "\n",
    "if 'query' in globals():\n",
    "    print(\"Arr√™t du streaming en cours...\")\n",
    "    query.stop()\n",
    "    print(\"‚úì Streaming arr√™t√© avec succ√®s\")\n",
    "else:\n",
    "    print(\"‚ö† Aucun streaming actif trouv√© (variable 'query' non d√©finie)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d705b05",
   "metadata": {},
   "source": [
    "## 4.1 - Transformation Silver des donn√©es streaming\n",
    "\n",
    "Traitement s√©par√© des donn√©es Kafka vers Silver, similaire √† la Phase 2 mais adapt√© aux donn√©es streaming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83598461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la m√™me SparkSession que la Phase 2\n",
    "# Si le streaming est toujours actif, on peut utiliser spark_streaming, sinon spark\n",
    "\n",
    "# Lecture des donn√©es Kafka depuis Bronze (apr√®s arr√™t du streaming)\n",
    "# Note: Pour un traitement en continu, on pourrait aussi faire un streaming vers Silver\n",
    "\n",
    "# S'assurer que ingestion_date est d√©fini (au cas o√π cette cellule est ex√©cut√©e seule)\n",
    "from datetime import datetime\n",
    "if 'ingestion_date' not in globals():\n",
    "    ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"[INFO] ingestion_date d√©fini automatiquement: {ingestion_date}\")\n",
    "\n",
    "# S'assurer que spark et F sont disponibles\n",
    "from pyspark.sql import functions as F\n",
    "if 'spark' not in globals():\n",
    "    print(\"[ERREUR] SparkSession 'spark' non trouv√©e. Veuillez ex√©cuter la Phase 1 d'abord.\")\n",
    "    raise NameError(\"SparkSession 'spark' non trouv√©e\")\n",
    "\n",
    "BRONZE_KAFKA_PATH = \"s3a://bronze/kafka_orders\"\n",
    "\n",
    "def process_kafka_orders_to_silver(ingestion_date: str):\n",
    "    \"\"\"Transforme les donn√©es Kafka de Bronze vers Silver.\"\"\"\n",
    "    try:\n",
    "        # Lecture depuis Bronze\n",
    "        df_kafka_bronze = spark.read.parquet(f\"{BRONZE_KAFKA_PATH}/*\")\n",
    "        \n",
    "        if df_kafka_bronze.count() == 0:\n",
    "            print(f\"[INFO] Aucune donn√©e Kafka trouv√©e dans {BRONZE_KAFKA_PATH}\")\n",
    "            return None\n",
    "        \n",
    "        # Transformation Silver : nettoyage et enrichissement\n",
    "        df_silver = (df_kafka_bronze\n",
    "            .withColumn(\"montant_net\", \n",
    "                       (F.col(\"unit_price\") * F.col(\"quantity\") * (F.lit(1.0) - F.col(\"discount\"))).cast(\"double\"))\n",
    "            .withColumn(\"order_date\", F.to_date(F.col(\"order_timestamp\")))\n",
    "            .select(\n",
    "                \"order_id\",\n",
    "                \"customer_id\",\n",
    "                \"product_id\",\n",
    "                \"quantity\",\n",
    "                \"unit_price\",\n",
    "                \"discount\",\n",
    "                \"montant_net\",\n",
    "                \"status\",\n",
    "                \"order_date\",\n",
    "                \"order_timestamp\",\n",
    "                \"_ingestion_timestamp\",\n",
    "                \"_source_system\",\n",
    "                \"_table_name\",\n",
    "                \"_ingestion_date\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # √âcriture en Silver\n",
    "        target_path = f\"s3a://silver/kafka_orders/{ingestion_date}\"\n",
    "        row_count = df_silver.count()\n",
    "        df_silver.write.mode(\"overwrite\").parquet(target_path)\n",
    "        \n",
    "        return {\n",
    "            \"dataset\": \"kafka_orders\",\n",
    "            \"rows\": row_count,\n",
    "            \"path\": target_path,\n",
    "            \"partition\": ingestion_date\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[ERREUR] Traitement Kafka vers Silver: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ex√©cution (d√©commentez apr√®s avoir arr√™t√© le streaming et laiss√© quelques donn√©es s'accumuler)\n",
    "# result_kafka_silver = process_kafka_orders_to_silver(ingestion_date)\n",
    "# if result_kafka_silver:\n",
    "#     print(f\"[OK] {result_kafka_silver['dataset']} -> {result_kafka_silver['rows']} lignes -> {result_kafka_silver['path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468772a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des donn√©es batch \n",
    "print(\"=== PHASE 4 - COMPARAISON BATCH vs STREAMING ===\\n\")\n",
    "\n",
    "# S'assurer que ingestion_date est d√©fini (au cas o√π cette cellule est ex√©cut√©e seule)\n",
    "from datetime import datetime\n",
    "if 'ingestion_date' not in globals():\n",
    "    ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"[INFO] ingestion_date d√©fini automatiquement: {ingestion_date}\")\n",
    "\n",
    "# S'assurer que spark et F sont disponibles\n",
    "from pyspark.sql import functions as F\n",
    "if 'spark' not in globals():\n",
    "    print(\"[ERREUR] SparkSession 'spark' non trouv√©e. Veuillez ex√©cuter la Phase 1 d'abord.\")\n",
    "    raise NameError(\"SparkSession 'spark' non trouv√©e\")\n",
    "\n",
    "# Red√©finition des chemins\n",
    "BRONZE_KAFKA_PATH = \"s3a://bronze/kafka_orders\"\n",
    "\n",
    "# Donn√©es batch (PostgreSQL)\n",
    "try:\n",
    "    df_batch_orders = spark.read.parquet(f\"s3a://bronze/orders/{ingestion_date}\")\n",
    "    batch_count = df_batch_orders.count()\n",
    "    print(f\"[BATCH] Commandes PostgreSQL: {batch_count} lignes\")\n",
    "    print(f\"       Source: postgresql\")\n",
    "    print(f\"       Path: s3a://bronze/orders/{ingestion_date}\")\n",
    "except Exception as e:\n",
    "    print(f\"[BATCH] Erreur: {e}\")\n",
    "    batch_count = 0\n",
    "\n",
    "print()\n",
    "\n",
    "# Donn√©es streaming (Kafka)\n",
    "try:\n",
    "    df_streaming_orders = spark.read.parquet(f\"{BRONZE_KAFKA_PATH}/*\")\n",
    "    streaming_count = df_streaming_orders.count()\n",
    "    print(f\"[STREAMING] Commandes Kafka: {streaming_count} lignes\")\n",
    "    print(f\"           Source: kafka\")\n",
    "    print(f\"           Path: {BRONZE_KAFKA_PATH}\")\n",
    "    if streaming_count > 0:\n",
    "        print(f\"           Derni√®re commande:\")\n",
    "        df_streaming_orders.orderBy(F.col(\"_ingestion_timestamp\").desc()).show(1, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"[STREAMING] Erreur: {e}\")\n",
    "    streaming_count = 0\n",
    "\n",
    "print(f\"\\nTotal commandes (batch + streaming): {batch_count + streaming_count}\")\n",
    "print(\"\\nLes deux sources coexistent et peuvent √™tre utilis√©es s√©par√©ment ou ensemble pour l'analyse.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f2cde",
   "metadata": {},
   "source": [
    "## Explication d√©taill√©e du choix : Option A vs Option B\n",
    "\n",
    "### Analyse des deux options\n",
    "\n",
    "#### Option A : Traitement s√©par√© (choix retenu)\n",
    "\n",
    "**Avantages :**\n",
    "1. **Tra√ßabilit√©** : Chaque source (PostgreSQL vs Kafka) conserve sa m√©tadonn√©e `_source_system`, permettant de savoir d'o√π viennent les donn√©es\n",
    "2. **Flexibilit√©** : Les traitements peuvent √™tre diff√©rents selon la source \n",
    "3. **Maintenance** : En cas de probl√®me, il est plus facile d'identifier la source et de corriger ind√©pendamment\n",
    "4. **Performance** : Les traitements batch et streaming peuvent √™tre optimis√©s s√©par√©ment\n",
    "5. **S√©paration des pr√©occupations** : Respect du principe SOLID dans l'architecture Medallion\n",
    "\n",
    "**Impl√©mentation :**\n",
    "- Donn√©es batch (Phase 1) ‚Üí Bronze ‚Üí Silver (Phase 2) : `fact_orders`, `dim_customers`, `dim_products`\n",
    "- Donn√©es streaming (Phase 3) ‚Üí Bronze ‚Üí Silver (Phase 4.1) : `kafka_orders`\n",
    "- Les deux sources coexistent et peuvent √™tre utilis√©es ensemble pour des analyses (Phase 4.2)\n",
    "\n",
    "#### Option B : Union batch + streaming (non retenue)\n",
    "\n",
    "**Avantages :**\n",
    "- Vue unifi√©e imm√©diate des donn√©es historiques et temps r√©el\n",
    "- Simplification des requ√™tes d'analyse (une seule table √† interroger)\n",
    "\n",
    "**Inconv√©nients :**\n",
    "- Perte de tra√ßabilit√© de la source\n",
    "- Complexit√© accrue dans la gestion des conflits (m√™me order_id dans batch et streaming)\n",
    "- Moins de flexibilit√© pour des traitements diff√©renci√©s\n",
    "- Plus difficile √† d√©boguer en cas de probl√®me\n",
    "\n",
    "### Justification du choix\n",
    "\n",
    "Nous avons choisi **l'Option A** car elle offre une meilleure architecture pour un data lake en production :\n",
    "\n",
    "1. **Conformit√© avec l'architecture Medallion** : Chaque couche (Bronze, Silver) peut traiter diff√©remment les sources selon leurs caract√©ristiques\n",
    "2. **Scalabilit√©** : Facilite l'ajout de nouvelles sources de donn√©es sans impacter les existantes\n",
    "3. **Qualit√© des donn√©es** : Permet d'appliquer des r√®gles de qualit√© diff√©rentes selon la source\n",
    "4. **Gouvernance** : Meilleure tra√ßabilit√© et auditabilit√© des donn√©es\n",
    "\n",
    "**Note** : L'Option B reste possible en cr√©ant une vue unifi√©e ou en faisant une union lors des analyses finales (couche Gold), mais nous pr√©f√©rons maintenir la s√©paration jusqu'√† cette √©tape pour pr√©server la tra√ßabilit√©.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
