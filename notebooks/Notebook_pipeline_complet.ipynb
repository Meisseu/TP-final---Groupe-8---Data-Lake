{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5404d5bd",
   "metadata": {},
   "source": [
    "# TP Final — Phase 1 : Ingestion Bronze (PostgreSQL → MinIO)\n",
    "\n",
    "**But** : ingerer des donnees brutes depuis PostgreSQL (Northwind) vers la zone **Bronze** (MinIO/S3) selon l’architecture Medallion.\n",
    "\n",
    "## Attendus couverts\n",
    "- Lecture generique d’une table PostgreSQL\n",
    "- Ecriture Parquet dans `s3a://bronze/<table_name>/`\n",
    "- Partitionnement par **date d’ingestion** au format `YYYY-MM-DD`\n",
    "- Ajout des metadonnees techniques obligatoires :\n",
    "  - `_ingestion_timestamp`\n",
    "  - `_source_system` = `postgresql`\n",
    "  - `_table_name`\n",
    "\n",
    "> Remarque : on ajoute aussi `_ingestion_date` pour porter la partition (colonne technique)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088c7fd",
   "metadata": {},
   "source": [
    "## 1) Contexte d’execution\n",
    "Ce notebook est fait pour etre execute **dans le JupyterLab de l’environnement Docker** (service `jupyter-spark`).\n",
    "\n",
    "- PostgreSQL dans Docker : `jdbc:postgresql://postgres:5432/app`\n",
    "- MinIO dans Docker : `http://minio:9000`\n",
    "- Buckets attendus : `bronze`, `silver`, `gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d58b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK\n",
      "S3A endpoint = http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# SparkSession avec config MinIO + drivers (au besoin)\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"TP Final - Phase 1 Bronze\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Pour permettre overwrite uniquement de la partition du jour\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "print(\"Spark OK\")\n",
    "print(\"S3A endpoint =\", spark.sparkContext._jsc.hadoopConfiguration().get(\"fs.s3a.endpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc3233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion date = 2026-01-16\n"
     ]
    }
   ],
   "source": [
    "# Configuration JDBC PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/app\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "# Date d’ingestion (partition) au format YYYY-MM-DD\n",
    "ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(\"Ingestion date =\", ingestion_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4330fb",
   "metadata": {},
   "source": [
    "## 2) Fonction d’ingestion generique (PostgreSQL → Bronze)\n",
    "Cette fonction :\n",
    "- lit la table via JDBC\n",
    "- ajoute les colonnes techniques obligatoires\n",
    "- ecrit en Parquet dans `s3a://bronze/<table_name>/<YYYY-MM-DD>/` (partitionnement par dossier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1b4cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_table_to_bronze(table_name: str, *, ingestion_date: str, base_path: str = \"s3a://bronze\") -> dict:\n",
    "    \"\"\"\n",
    "    Ingerer une table PostgreSQL vers la zone Bronze sur MinIO.\n",
    "\n",
    "    Ecrit en Parquet et partitionne par date d’ingestion (YYYY-MM-DD) via la structure de dossiers.\n",
    "    Ajoute les metadonnees techniques :\n",
    "      - _ingestion_timestamp\n",
    "      - _source_system = 'postgresql'\n",
    "      - _table_name\n",
    "      - _ingestion_date\n",
    "\n",
    "    Retourne un petit dictionnaire de stats.\n",
    "    \"\"\"\n",
    "    ingestion_ts_col = F.current_timestamp()\n",
    "\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_properties)\n",
    "    row_count = df.count()\n",
    "\n",
    "    df_out = (df\n",
    "        .withColumn(\"_ingestion_timestamp\", ingestion_ts_col)\n",
    "        .withColumn(\"_source_system\", F.lit(\"postgresql\"))\n",
    "        .withColumn(\"_table_name\", F.lit(table_name))\n",
    "        .withColumn(\"_ingestion_date\", F.lit(ingestion_date))\n",
    "    )\n",
    "\n",
    "    target_path = f\"{base_path}/{table_name}/{ingestion_date}\"\n",
    "\n",
    "    df_out.write.mode(\"overwrite\").parquet(target_path)\n",
    "\n",
    "    return {\n",
    "        \"table\": table_name,\n",
    "        \"rows\": row_count,\n",
    "        \"path\": target_path,\n",
    "        \"partition\": ingestion_date,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fcc38",
   "metadata": {},
   "source": [
    "## 3) Execution Phase 1 : tables obligatoires (+ bonus)\n",
    "Tables obligatoires : `customers`, `orders`, `order_details`, `products`\n",
    "\n",
    "Tables bonus (si dispo) : `employees`, `suppliers`, `categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3892084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1 - INGESTION BRONZE ===\n",
      "[OK] customers -> 91 lignes -> s3a://bronze/customers/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] orders -> 830 lignes -> s3a://bronze/orders/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] order_details -> 2155 lignes -> s3a://bronze/order_details/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] products -> 77 lignes -> s3a://bronze/products/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] employees -> 9 lignes -> s3a://bronze/employees/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] suppliers -> 29 lignes -> s3a://bronze/suppliers/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "[OK] categories -> 8 lignes -> s3a://bronze/categories/2026-01-16 (_ingestion_date=2026-01-16)\n",
      "\n",
      "Resume OK :\n",
      "  - customers: 91 lignes\n",
      "  - orders: 830 lignes\n",
      "  - order_details: 2155 lignes\n",
      "  - products: 77 lignes\n",
      "  - employees: 9 lignes\n",
      "  - suppliers: 29 lignes\n",
      "  - categories: 8 lignes\n"
     ]
    }
   ],
   "source": [
    "required_tables = [\"customers\", \"orders\", \"order_details\", \"products\"]\n",
    "bonus_tables = [\"employees\", \"suppliers\", \"categories\"]\n",
    "tables_to_ingest = required_tables + bonus_tables\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "print(\"=== PHASE 1 - INGESTION BRONZE ===\")\n",
    "for table in tables_to_ingest:\n",
    "    try:\n",
    "        stats = ingest_table_to_bronze(table, ingestion_date=ingestion_date)\n",
    "        results.append(stats)\n",
    "        print(f\"[OK] {stats['table']} -> {stats['rows']} lignes -> {stats['path']} (_ingestion_date={stats['partition']})\")\n",
    "    except Exception as e:\n",
    "        errors.append({\"table\": table, \"error\": str(e)})\n",
    "        print(f\"[ERREUR] {table} : {e}\")\n",
    "\n",
    "print(\"\\nResume OK :\")\n",
    "for r in results:\n",
    "    print(f\"  - {r['table']}: {r['rows']} lignes\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nResume erreurs :\")\n",
    "    for err in errors:\n",
    "        print(f\"  - {err['table']}: {err['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329dd24",
   "metadata": {},
   "source": [
    "## 4) Verifications rapides\n",
    "On relit une table depuis Bronze et on verifie la presence des colonnes techniques + la partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97f623ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes manquantes : set()\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "|_ingestion_timestamp      |_source_system|_table_name|_ingestion_date|\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "|2026-01-16 12:41:52.633452|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 12:41:52.633452|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 12:41:52.633452|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 12:41:52.633452|postgresql    |customers  |2026-01-16     |\n",
      "|2026-01-16 12:41:52.633452|postgresql    |customers  |2026-01-16     |\n",
      "+--------------------------+--------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "+---------------+-----+\n",
      "|_ingestion_date|count|\n",
      "+---------------+-----+\n",
      "|2026-01-16     |91   |\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple : verifier customers (lecture de la partition du jour)\n",
    "df_bronze_customers = spark.read.parquet(f\"s3a://bronze/customers/{ingestion_date}\")\n",
    "\n",
    "expected_cols = {\"_ingestion_timestamp\", \"_source_system\", \"_table_name\", \"_ingestion_date\"}\n",
    "missing = expected_cols - set(df_bronze_customers.columns)\n",
    "print(\"Colonnes manquantes :\", missing)\n",
    "\n",
    "df_bronze_customers.select(\n",
    "    \"_ingestion_timestamp\",\n",
    "    \"_source_system\",\n",
    "    \"_table_name\",\n",
    "    \"_ingestion_date\",\n",
    ").show(5, truncate=False)\n",
    "\n",
    "df_bronze_customers.groupBy(\"_ingestion_date\").count().orderBy(\"_ingestion_date\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc3401",
   "metadata": {},
   "source": [
    "# TP Final — Phase 2 : Raffinement Silver (Bronze → Silver)\n",
    "\n",
    "Objectif : nettoyer, typer et structurer les donnees Bronze pour les rendre exploitables en Silver.\n",
    "\n",
    "Sorties attendues (minimum) :\n",
    "- `dim_customers` : `company_name` en InitCap, `country` en MAJUSCULES\n",
    "- `dim_products` : jointure produits ↔ categories + `stock_status` (CRITIQUE si stock < 10, sinon NORMAL)\n",
    "- `fact_orders` : jointure `orders` + `order_details` + calcul `montant_net`\n",
    "\n",
    "Ecriture en Parquet sous `s3a://silver/<dataset>/<YYYY-MM-DD>/` (meme partition jour que la Phase 1).\n",
    "\n",
    "> Important : cette Phase 2 lit la partition Bronze du jour `s3a://bronze/<table>/<YYYY-MM-DD>/` produite par la Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a62e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def _first_existing_column(df, candidates: list[str]) -> str:\n",
    "    for name in candidates:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    raise ValueError(f\"Aucune colonne trouvee parmi {candidates}. Colonnes dispo: {df.columns}\")\n",
    "\n",
    "def _canonicalize_columns(df, mapping: dict[str, list[str]]):\n",
    "    # mapping: canonical_name -> [candidates...]\n",
    "    out = df\n",
    "    for canonical, candidates in mapping.items():\n",
    "        found = _first_existing_column(out, candidates)\n",
    "        if found != canonical:\n",
    "            out = out.withColumnRenamed(found, canonical)\n",
    "    return out\n",
    "\n",
    "def read_bronze(table_name: str, ingestion_date: str):\n",
    "    path = f\"s3a://bronze/{table_name}/{ingestion_date}\"\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def write_silver(df, dataset_name: str, ingestion_date: str, base_path: str = \"s3a://silver\") -> dict:\n",
    "    target_path = f\"{base_path}/{dataset_name}/{ingestion_date}\"\n",
    "    row_count = df.count()\n",
    "    df.write.mode(\"overwrite\").parquet(target_path)\n",
    "    return {\"dataset\": dataset_name, \"rows\": row_count, \"path\": target_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b663ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 2 - ECRITURE SILVER OK ===\n",
      "[OK] dim_customers -> 91 lignes -> s3a://silver/dim_customers/2026-01-16\n",
      "[OK] dim_products -> 77 lignes -> s3a://silver/dim_products/2026-01-16\n",
      "[OK] fact_orders -> 2155 lignes -> s3a://silver/fact_orders/2026-01-16\n"
     ]
    }
   ],
   "source": [
    "# --- Lecture Bronze (partition du jour) ---\n",
    "customers_bronze = read_bronze(\"customers\", ingestion_date)\n",
    "orders_bronze = read_bronze(\"orders\", ingestion_date)\n",
    "order_details_bronze = read_bronze(\"order_details\", ingestion_date)\n",
    "products_bronze = read_bronze(\"products\", ingestion_date)\n",
    "\n",
    "# Tables bonus (si presentes) : categories (pour dim_products)\n",
    "categories_bronze = None\n",
    "try:\n",
    "    categories_bronze = read_bronze(\"categories\", ingestion_date)\n",
    "except Exception as e:\n",
    "    print(\"[INFO] categories non disponible en Bronze:\", e)\n",
    "\n",
    "# --- 2.1 Dim_Customers ---\n",
    "# Règles : company_name InitCap, country UPPER + dedup clé métier\n",
    "customers = _canonicalize_columns(customers_bronze, {\n",
    "    \"customer_id\": [\"customer_id\", \"customerid\", \"CustomerID\"],\n",
    "    \"company_name\": [\"company_name\", \"companyname\", \"CompanyName\"],\n",
    "    \"country\": [\"country\", \"Country\"],\n",
    "})\n",
    "dim_customers = (customers\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "    .withColumn(\"company_name\", F.initcap(F.col(\"company_name\")))\n",
    "    .withColumn(\"country\", F.upper(F.col(\"country\")))\n",
    "    .select(\"customer_id\", \"company_name\", \"country\")\n",
    " )\n",
    "\n",
    "# --- 2.2 Dim_Products ---\n",
    "# Règles : join produits ↔ categories + stock_status\n",
    "products = _canonicalize_columns(products_bronze, {\n",
    "    \"product_id\": [\"product_id\", \"productid\", \"ProductID\"],\n",
    "    \"product_name\": [\"product_name\", \"productname\", \"ProductName\"],\n",
    "    \"category_id\": [\"category_id\", \"categoryid\", \"CategoryID\"],\n",
    "    \"units_in_stock\": [\"units_in_stock\", \"unitsinstock\", \"UnitsInStock\"],\n",
    "})\n",
    "products = (products\n",
    "    .withColumn(\"units_in_stock\", F.col(\"units_in_stock\").cast(\"int\"))\n",
    "    .dropDuplicates([\"product_id\"])\n",
    " )\n",
    "if categories_bronze is not None:\n",
    "    categories = _canonicalize_columns(categories_bronze, {\n",
    "        \"category_id\": [\"category_id\", \"categoryid\", \"CategoryID\"],\n",
    "        \"category_name\": [\"category_name\", \"categoryname\", \"CategoryName\"],\n",
    "    }).dropDuplicates([\"category_id\"])\n",
    "    products = products.join(categories.select(\"category_id\", \"category_name\"), on=\"category_id\", how=\"left\")\n",
    "else:\n",
    "    products = products.withColumn(\"category_name\", F.lit(None).cast(\"string\"))\n",
    "dim_products = (products\n",
    "    .withColumn(\"stock_status\", F.when(F.col(\"units_in_stock\") < 10, F.lit(\"CRITIQUE\")).otherwise(F.lit(\"NORMAL\")))\n",
    "    .select(\"product_id\", \"product_name\", \"category_id\", \"category_name\", \"units_in_stock\", \"stock_status\")\n",
    " )\n",
    "\n",
    "# --- 2.3 Fact_Orders ---\n",
    "# Règles : jointure orders + order_details + montant_net\n",
    "orders = _canonicalize_columns(orders_bronze, {\n",
    "    \"order_id\": [\"order_id\", \"orderid\", \"OrderID\"],\n",
    "    \"customer_id\": [\"customer_id\", \"customerid\", \"CustomerID\"],\n",
    "    \"order_date\": [\"order_date\", \"orderdate\", \"OrderDate\"],\n",
    "})\n",
    "order_details = _canonicalize_columns(order_details_bronze, {\n",
    "    \"order_id\": [\"order_id\", \"orderid\", \"OrderID\"],\n",
    "    \"product_id\": [\"product_id\", \"productid\", \"ProductID\"],\n",
    "    \"unit_price\": [\"unit_price\", \"unitprice\", \"UnitPrice\"],\n",
    "    \"quantity\": [\"quantity\", \"Quantity\"],\n",
    "    \"discount\": [\"discount\", \"Discount\"],\n",
    "})\n",
    "orders = orders.withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "order_details = (order_details\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"discount\", F.col(\"discount\").cast(\"double\"))\n",
    "    .dropDuplicates([\"order_id\", \"product_id\"])\n",
    " )\n",
    "fact_orders = (orders.select(\"order_id\", \"customer_id\", \"order_date\")\n",
    "    .join(order_details, on=\"order_id\", how=\"inner\")\n",
    "    .withColumn(\"montant_net\", (F.col(\"unit_price\") * F.col(\"quantity\") * (F.lit(1.0) - F.col(\"discount\"))).cast(\"double\"))\n",
    "    .select(\"order_id\", \"product_id\", \"customer_id\", \"order_date\", \"quantity\", \"unit_price\", \"discount\", \"montant_net\")\n",
    " )\n",
    "\n",
    "# --- Ecriture Silver ---\n",
    "results_silver = [\n",
    "    write_silver(dim_customers, \"dim_customers\", ingestion_date),\n",
    "    write_silver(dim_products, \"dim_products\", ingestion_date),\n",
    "    write_silver(fact_orders, \"fact_orders\", ingestion_date),\n",
    "]\n",
    "print(\"=== PHASE 2 - ECRITURE SILVER OK ===\")\n",
    "for r in results_silver:\n",
    "    print(f\"[OK] {r['dataset']} -> {r['rows']} lignes -> {r['path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25bc0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------+-------+\n",
      "|customer_id|company_name                      |country|\n",
      "+-----------+----------------------------------+-------+\n",
      "|ALFKI      |Alfreds Futterkiste               |GERMANY|\n",
      "|ANATR      |Ana Trujillo Emparedados Y Helados|MEXICO |\n",
      "|ANTON      |Antonio Moreno Taquería           |MEXICO |\n",
      "|AROUT      |Around The Horn                   |UK     |\n",
      "|BERGS      |Berglunds Snabbköp                |SWEDEN |\n",
      "+-----------+----------------------------------+-------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verification rapide : relire une sortie Silver\n",
    "df_check_dim_customers = spark.read.parquet(f\"s3a://silver/dim_customers/{ingestion_date}\")\n",
    "df_check_dim_customers.show(5, truncate=False)\n",
    "df_check_dim_customers.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
