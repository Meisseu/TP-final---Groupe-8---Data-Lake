{"cells":[{"cell_type":"markdown","metadata":{},"source":["# TP Final — Phase 2 : Transformation Silver (Bronze → Silver)\n","\n","Ce notebook transforme les données brutes de la zone Bronze en jeux de données Silver nettoyés et enrichis.\n","Principes appliqués :\n","- Nettoyage et typage des colonnes\n","- Déduplication\n","- Jointures pour enrichissement (ex : produits ↔ suppliers, categories)\n","- Ajout des métadonnées techniques obligatoires : `_ingestion_timestamp`, `_source_system`, `_table_name`, `_ingestion_date`\n","- Écriture en Parquet dans `s3a://silver/<table>/<ingestion_date>/`\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","from datetime import datetime\n","\n","# SparkSession (config MinIO + drivers si besoin)\n","spark = (SparkSession.builder\n","    .appName(\"TP Final - Phase 2 Silver\")\n","    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0,org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n","    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n","    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n","    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n","    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n","    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n","    .getOrCreate())\n","\n","spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n","print(\"Spark OK\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"source":["# Date d'ingestion (on traite la partition du jour, même logique que Phase 1)\n","ingestion_date = datetime.now().strftime(\"%Y-%m-%d\")\n","print(\"Ingestion date =\", ingestion_date)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Fonctions utilitaires\n","On écrit quelques fonctions réutilisables pour lire depuis Bronze et ecrire vers Silver en ajoutant les métadonnées techniques.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"source":["def read_bronze(table_name: str, ingestion_date: str) -> 'DataFrame':\n","    \"\"\"Lit la partition Bronze correspondant à `ingestion_date`.\n","    Si la partition n'existe pas, lève une exception (comportement explicite).\n","    \"\"\"\n","    path = f's3a://bronze/{table_name}/{ingestion_date}'\n","    return spark.read.parquet(path)\n","\n","def write_silver(df, table_name: str, ingestion_date: str, base_path: str = 's3a://silver') -> dict:\n","    \"\"\"Ajoute métadonnées techniques et écrit en Parquet sous base_path/table_name/ingestion_date\n","    Retourne des stats simples.\n","    \"\"\"\n","    df_out = (df\n","        .withColumn('_ingestion_timestamp', F.current_timestamp())\n","        .withColumn('_source_system', F.lit('bronze'))\n","        .withColumn('_table_name', F.lit(table_name))\n","        .withColumn('_ingestion_date', F.lit(ingestion_date))\n","    )\n","    target_path = f'{base_path}/{table_name}/{ingestion_date}'\n","    row_count = df_out.count()\n","    df_out.write.mode('overwrite').parquet(target_path)\n","    return {'table': table_name, 'rows': row_count, 'path': target_path, 'partition': ingestion_date}\n"]},{"cell_type":"markdown","metadata":{},"source":["## Transformations par table\n","On applique des règles simples mais pédagogiques pour produire des tables Silver utiles pour l'analyse.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"source":["def customers_silver(ingestion_date: str):\n","    df = read_bronze('customers', ingestion_date)\n","    # Exemple de nettoyage : trim, normalisation email, remplacer valeurs vides par NULL\n","    df2 = (df\n","        .withColumn('contact_name', F.trim(F.col('contact_name'))).\n","        .withColumn('company_name', F.trim(F.col('company_name'))).\n","        .withColumn('contact_title', F.trim(F.col('contact_title'))).\n","        .withColumn('address', F.when(F.trim(F.col('address')) == '', None).otherwise(F.col('address'))).\n","        .withColumn('city', F.trim(F.col('city'))).\n","        .withColumn('country', F.trim(F.col('country'))).\n","    )\n","    # Deduplication sur customerid (clé primaire attendue)\n","    df2 = df2.dropDuplicates(['customerid'])\n","    # Sélectionner et renommer colonnes pour la zone Silver\n","    selected = df2.select('customerid', 'company_name', 'contact_name', 'contact_title', 'address', 'city', 'region', 'postalcode', 'country', 'phone', 'fax')\n","    return selected\n","\n","def products_silver(ingestion_date: str):\n","    df = read_bronze('products', ingestion_date)\n","    # cast des colonnes numériques et nettoyage\n","    df2 = (df\n","        .withColumn('unitprice', F.col('unitprice').cast(T.DoubleType())).\n","        .withColumn('quantityperunit', F.trim(F.col('quantityperunit'))).\n","        .withColumn('discontinued', F.col('discontinued').cast(T.IntegerType())).\n","    )\n","    # Enrichir avec suppliers et categories si disponibles (lecture depuis Bronze)\n","    try:\n","        suppliers = read_bronze('suppliers', ingestion_date).select('supplierid', F.col('companyname').alias('supplier_name'))\n","        df2 = df2.join(suppliers, on='supplierid', how='left')\n","    except Exception:\n","        pass\n","    try:\n","        categories = read_bronze('categories', ingestion_date).select('categoryid', F.col('categoryname').alias('category_name'))\n","        df2 = df2.join(categories, on='categoryid', how='left')\n","    except Exception:\n","        pass\n","    selected = df2.select('productid', 'productname', 'supplierid', 'supplier_name', 'categoryid', 'category_name', 'quantityperunit', 'unitprice', 'unitsinstock', 'unitsonorder', 'reorderlevel', 'discontinued')\n","    return selected\n","\n","def order_items_silver(ingestion_date: str):\n","    df_od = read_bronze('order_details', ingestion_date)\n","    # joindre products pour obtenir productname et unitprice de la zone Silver\n","    try:\n","        products = read_bronze('products', ingestion_date).select('productid', 'productname', 'unitprice')\n","        df2 = df_od.join(products, on='productid', how='left')\n","    except Exception:\n","        df2 = df_od\n","    # calculer total par ligne\n","    df2 = df2.withColumn('line_total', (F.col('unitprice') * F.col('quantity') * (1 - F.col('discount'))).cast(T.DoubleType()))\n","    selected = df2.select('orderid', 'productid', 'unitprice', 'quantity', 'discount', 'line_total')\n","    return selected\n","\n","def orders_silver(ingestion_date: str):\n","    df_orders = read_bronze('orders', ingestion_date)\n","    # caster les dates et calculer des flags simples\n","    df2 = (df_orders\n","        .withColumn('order_date', F.to_date(F.col('orderdate'))).\n","        .withColumn('required_date', F.to_date(F.col('requireddate'))).\n","        .withColumn('shipped_date', F.to_date(F.col('shippeddate'))).\n","    )\n","    # joindre customers pour quelques attributs\n","    try:\n","        customers = read_bronze('customers', ingestion_date).select('customerid', F.col('company_name').alias('company_name'))\n","        df2 = df2.join(customers, on='customerid', how='left')\n","    except Exception:\n","        pass\n","    selected = df2.select('orderid', 'customerid', 'company_name', 'employeeid', 'order_date', 'required_date', 'shipped_date', 'freight', 'shipname', 'shipaddress', 'shipcity', 'shipregion', 'shippostalcode', 'shipcountry')\n","    return selected\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exécution des transformations (partition du jour)\n","On applique les transformations pour les tables principales.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"source":["tables_and_funcs = [\n","    ('customers', customers_silver),\n","    ('products', products_silver),\n","    ('order_items', order_items_silver),\n","    ('orders', orders_silver),\n","]\n","results = []\n","errors = []\n","print('=== PHASE 2 - TRANSFORMATIONS SILVER ===')\n","for name, func in tables_and_funcs:\n","    try:\n","        df_s = func(ingestion_date)\n","        stats = write_silver(df_s, name, ingestion_date)\n","        results.append(stats)\n","        print(f"[OK] {name} -> {stats['rows']} lignes -> {stats['path']}")\n","    except Exception as e:\n","        errors.append({'table': name, 'error': str(e)})\n","        print(f"[ERREUR] {name} : {e}")\n","\n","print('\nResume OK :')\n","for r in results:\n","    print(f"  - {r['table']}: {r['rows']} lignes")\n","if errors:\n","    print('\nResume erreurs :')\n","    for err in errors:\n","        print(f"  - {err['table']}: {err['error']}")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Vérifications rapides\n","On relit un exemple (customers) depuis Silver pour vérifier la présence des métadonnées.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"source":["# Lecture d'un exemple et affichage\n","df_customers_silver = spark.read.parquet(f's3a://silver/customers/{ingestion_date}')\n","df_customers_silver.select('_ingestion_timestamp', '_source_system', '_table_name', '_ingestion_date').show(5, truncate=False)\n","df_customers_silver.printSchema()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}